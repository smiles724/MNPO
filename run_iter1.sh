set -e

###########################################
# Path Settings
###########################################

# Root directory of your workspace

PROJECT_ROOT=/path/to/your/project

# Local cache directories
CACHE_DIR=$PROJECT_ROOT/cache
HF_CACHE_DIR=$PROJECT_ROOT/hf_cache

export HF_HOME=$HF_CACHE_DIR

###########################################
# Python environments
###########################################
# mnpo_infer  → for decoding + post-processing
# mnpo_train → for Armo reward model + precompute

PYTHON_INFER=python        # e.g., /path/to/miniconda/envs/mnpo_infer/bin/python
PYTHON_TRAIN=python        # e.g., /path/to/miniconda/envs/mnpo_train/bin/python

###########################################
# Experiment Settings
###########################################

# Policy and reference models
POLICY_MODEL=$PROJECT_ROOT/outputs/gemma-2-9b-it_mnpo_stage_1_armo
REF_MODEL=$CACHE_DIR/gemma-2-9b-it       # Reference model path

# Dataset split for decoding
DATA_SPLIT=$PROJECT_ROOT/data/gemma2_ufb_part1_train.jsonl

# Output directory for decode + post_process
GEN_DIR=$PROJECT_ROOT/datasets/gemma2_ultrafeedback/mnpo_iter1_armo

# Scored output generated by Armo (input to precompute)
SCORED_FILE=${GEN_DIR}_scored.jsonl

# Output directory for precompute (HF dataset format)
PREF_DIR=$PROJECT_ROOT/data/mnpo_iter1_armo/pref


###########################################
# 3. Reward model scoring (skip for Armo, as scores are included in the dataset)
###########################################
$PYTHON_TRAIN -u -m on_policy_data_gen.rm_armo_new \
    --cache_dir "$CACHE_DIR" \
    --input_file "$DATA_SPLIT" \
    --output_file "$SCORED_FILE"

###########################################
# 4. Precompute MNPO log-probabilities
###########################################
$PYTHON_TRAIN -m accelerate.commands.launch --num_processes=2 -m mnpo_scripts.precompute \
    --model_name_or_path "$POLICY_MODEL" \
    --ref_model "$REF_MODEL" \
    --train_dir "$SCORED_FILE" \
    --output_dir "$PREF_DIR" \
    --history_paths \
        "$POLICY_MODEL" \
    --cache_dir "$CACHE_DIR" \
    --sanity_check False

# train
ACCELERATE_LOG_LEVEL=info $PYTHON_TRAIN -m accelerate.commands.launch \
    --config_file accelerate_configs/deepspeed_zero3.yaml \
    -m mnpo_scripts.run_mnpo \
    "training_configs/gemma-2-9b-it-mnpo-iter1-armo.yaml"
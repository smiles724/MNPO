set -e

###########################################
# Path Settings
###########################################

# Root directory of your workspace

PROJECT_ROOT=/path/to/your/project

# Local cache directories
CACHE_DIR=$PROJECT_ROOT/cache
HF_CACHE_DIR=$PROJECT_ROOT/hf_cache

export HF_HOME=$HF_CACHE_DIR

###########################################
# Python environments
###########################################
# mnpo_infer  → for decoding + post-processing
# mnpo_train → for Armo reward model + precompute

PYTHON_INFER=python        # e.g., /path/to/miniconda/envs/mnpo_infer/bin/python
PYTHON_TRAIN=python        # e.g., /path/to/miniconda/envs/mnpo_train/bin/python

###########################################
# Experiment Settings
###########################################

# Policy and reference models
POLICY_MODEL=$PROJECT_ROOT/outputs/gemma-2-9b-it_mnpo_stage_3_armo
POLICY_MODEL_T_2=$PROJECT_ROOT/outputs/gemma-2-9b-it_mnpo_stage_2_armo
POLICY_MODEL_T_3=$PROJECT_ROOT/outputs/gemma-2-9b-it_mnpo_stage_1_armo
REF_MODEL=$CACHE_DIR/gemma-2-9b-it       # Reference model path

# Dataset split for decoding
DATA_SPLIT=$PROJECT_ROOT/data/gemma2_ufb_part1_20k/gemma2_ufb_part1_split3.jsonl

# Output directory for decode + post_process
GEN_DIR=$PROJECT_ROOT/datasets/gemma2_ultrafeedback/mnpo_iter3_armo

# Scored output generated by Armo (input to precompute)
SCORED_FILE=${GEN_DIR}_scored.jsonl

# Output directory for precompute (HF dataset format)
PREF_DIR=$PROJECT_ROOT/data/mnpo_iter3_armo/pref

###########################################
# 1. Decode (using policy models)
###########################################
stdbuf -oL -eL $PYTHON_INFER -u -m on_policy_data_gen.decode \
    --data_dir "$DATA_SPLIT" \
    --model "$POLICY_MODEL" \
    --seeds 13 21 42 79 100 \
    --output_dir "$GEN_DIR" \
    --cache_dir "$CACHE_DIR" \
    --num_gpu 2

###########################################
# 2. Post-process decoded outputs
###########################################
$PYTHON_INFER -m on_policy_data_gen.post_process \
    --generation_file_dir "$GEN_DIR"

###########################################
# 3. Reward model scoring
###########################################
$PYTHON_TRAIN -u -m on_policy_data_gen.rm_armo_new \
    --cache_dir "$CACHE_DIR" \
    --input_file "$GEN_DIR/all_outputs.json" \
    --output_file "$SCORED_FILE"

###########################################
# 4. Precompute MNPO log-probabilities
###########################################
$PYTHON_TRAIN -m accelerate.commands.launch --num_processes=2 -m mnpo_scripts.precompute \
    --model_name_or_path "$POLICY_MODEL" \
    --ref_model "$REF_MODEL" \
    --train_dir "$SCORED_FILE" \
    --output_dir "$PREF_DIR" \
    --history_paths \
        "$POLICY_MODEL" \
        "$POLICY_MODEL_T_2" \
        "$POLICY_MODEL_T_3" \
    --cache_dir "$CACHE_DIR" \
    --sanity_check False

# train
ACCELERATE_LOG_LEVEL=info $PYTHON_TRAIN -m accelerate.commands.launch \
    --config_file accelerate_configs/deepspeed_zero3.yaml \
    -m mnpo_scripts.run_mnpo \
    "training_configs/gemma-2-9b-it-mnpo-iter3-armo.yaml"